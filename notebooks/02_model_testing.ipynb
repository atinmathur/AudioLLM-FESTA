{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen2-Audio Model Testing\n",
    "\n",
    "This notebook tests the Qwen2-Audio-7B-Instruct model on TREA dataset samples.\n",
    "\n",
    "## Contents\n",
    "1. Load model and dataset\n",
    "2. Test basic inference\n",
    "3. Analyze prediction patterns\n",
    "4. Compute baseline accuracy\n",
    "5. Test FES and FCS generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from IPython.display import Audio, display\n",
    "import librosa\n",
    "\n",
    "from src.data_loader import load_trea_dataset\n",
    "from src.model_wrapper import Qwen2AudioWrapper\n",
    "from src.fes_generator import FESGenerator\n",
    "from src.fcs_generator import FCSGenerator\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (small subset for testing)\n",
    "dataset = load_trea_dataset(\n",
    "    data_dir='../TREA_dataset',\n",
    "    tasks=['count', 'order', 'duration'],\n",
    "    samples_per_task=10,  # Start with 10 samples per task\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "# NOTE: This requires ~14GB GPU memory for 7B model\n",
    "# Set device='cpu' if GPU not available (will be slower)\n",
    "\n",
    "model = Qwen2AudioWrapper(\n",
    "    model_name=\"Qwen/Qwen2-Audio-7B-Instruct\",\n",
    "    device=\"cuda\",  # Change to \"cpu\" if no GPU\n",
    "    dtype=\"float16\",  # Use \"float32\" for CPU\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model info:\")\n",
    "info = model.get_model_info()\n",
    "for key, value in info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample\n",
    "sample = dataset.data[0]\n",
    "\n",
    "print(f\"Task: {sample['task'].upper()}\")\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"\\nOptions:\")\n",
    "for key, value in sample['options'].items():\n",
    "    print(f\"  ({key}) {value}\")\n",
    "print(f\"\\nGround Truth: {sample['correct_answer']}\")\n",
    "\n",
    "# Play audio\n",
    "audio, sr = librosa.load(sample['audio_path'], sr=16000)\n",
    "print(f\"\\nAudio (duration: {len(audio)/sr:.2f}s):\")\n",
    "display(Audio(audio, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model prediction\n",
    "prediction, probs = model.predict(\n",
    "    sample['audio_path'],\n",
    "    sample['question'],\n",
    "    sample['options'],\n",
    "    return_probs=True\n",
    ")\n",
    "\n",
    "print(f\"Model Prediction: {prediction}\")\n",
    "print(f\"Correct: {'✓ YES' if prediction == sample['correct_answer'] else '✗ NO'}\")\n",
    "\n",
    "print(f\"\\nProbabilities:\")\n",
    "for option, prob in sorted(probs.items()):\n",
    "    bar = '█' * int(prob * 50)\n",
    "    marker = '←' if option == prediction else ''\n",
    "    print(f\"  ({option}) {prob:.4f} {bar} {marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for all samples\n",
    "results = []\n",
    "\n",
    "for sample in tqdm(dataset.data, desc=\"Getting predictions\"):\n",
    "    prediction, _ = model.predict(\n",
    "        sample['audio_path'],\n",
    "        sample['question'],\n",
    "        sample['options']\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'task': sample['task'],\n",
    "        'prediction': prediction,\n",
    "        'ground_truth': sample['correct_answer'],\n",
    "        'correct': prediction == sample['correct_answer']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nCompleted {len(results)} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Accuracy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy\n",
    "overall_acc = results_df['correct'].mean()\n",
    "print(f\"Overall Accuracy: {overall_acc:.2%}\")\n",
    "\n",
    "# Task-wise accuracy\n",
    "print(f\"\\nTask-wise Accuracy:\")\n",
    "task_acc = results_df.groupby('task')['correct'].mean()\n",
    "for task, acc in task_acc.items():\n",
    "    print(f\"  {task}: {acc:.2%}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Task-wise accuracy\n",
    "task_acc.plot(kind='bar', ax=axes[0], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0].set_title('Accuracy by Task', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_xlabel('Task')\n",
    "axes[0].axhline(y=0.25, color='red', linestyle='--', label='Random (25%)')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=0)\n",
    "\n",
    "# Confusion matrix-style\n",
    "pred_counts = results_df.groupby(['task', 'correct']).size().unstack(fill_value=0)\n",
    "pred_counts.plot(kind='bar', stacked=True, ax=axes[1], color=['#FF6B6B', '#95E1D3'])\n",
    "axes[1].set_title('Correct vs Incorrect by Task', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xlabel('Task')\n",
    "axes[1].legend(['Incorrect', 'Correct'])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction distribution\n",
    "pred_dist = results_df['prediction'].value_counts()\n",
    "gt_dist = results_df['ground_truth'].value_counts()\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(pred_dist))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, pred_dist.values, width, label='Predictions', alpha=0.8)\n",
    "ax.bar(x + width/2, gt_dist.reindex(pred_dist.index, fill_value=0).values, \n",
    "       width, label='Ground Truth', alpha=0.8)\n",
    "\n",
    "ax.set_title('Prediction vs Ground Truth Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Answer Option', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(pred_dist.index)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test FES Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FES generator\n",
    "fes_gen = FESGenerator(\n",
    "    n_audio_samples=5,  # Small number for testing\n",
    "    n_text_samples=3,\n",
    "    sr=16000\n",
    ")\n",
    "\n",
    "# Generate FES samples for first sample\n",
    "sample = dataset.data[0]\n",
    "fes_samples = fes_gen.generate(\n",
    "    sample['audio_path'],\n",
    "    sample['question'],\n",
    "    sample['task'],\n",
    "    sample['options']\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(fes_samples)} FES samples\")\n",
    "print(f\"\\nOriginal question: {sample['question']}\")\n",
    "print(f\"\\nFES text variations:\")\n",
    "unique_questions = set(s['question'] for s in fes_samples)\n",
    "for i, q in enumerate(unique_questions, 1):\n",
    "    print(f\"  {i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FES predictions\n",
    "print(\"Getting predictions on FES samples...\")\n",
    "fes_predictions = []\n",
    "\n",
    "for fes_sample in tqdm(fes_samples[:10], desc=\"FES predictions\"):  # Test on subset\n",
    "    pred, _ = model.predict(\n",
    "        fes_sample['audio_path'],\n",
    "        fes_sample['question'],\n",
    "        fes_sample['options']\n",
    "    )\n",
    "    fes_predictions.append(pred)\n",
    "\n",
    "# Analyze consistency\n",
    "original_pred, _ = model.predict(\n",
    "    sample['audio_path'],\n",
    "    sample['question'],\n",
    "    sample['options']\n",
    ")\n",
    "\n",
    "print(f\"\\nOriginal prediction: {original_pred}\")\n",
    "print(f\"FES predictions: {Counter(fes_predictions)}\")\n",
    "\n",
    "consistency = sum(1 for p in fes_predictions if p == original_pred) / len(fes_predictions)\n",
    "print(f\"\\nConsistency: {consistency:.1%}\")\n",
    "print(f\"Model is {'CONSISTENT' if consistency > 0.7 else 'INCONSISTENT'} on FES samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test FCS Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FCS generator\n",
    "fcs_gen = FCSGenerator(\n",
    "    n_audio_samples=5,\n",
    "    n_text_samples=3,\n",
    "    sr=16000,\n",
    "    synthetic_silence_dir='../TREA_dataset/synthetic_silences'\n",
    ")\n",
    "\n",
    "# Generate FCS samples\n",
    "fcs_samples = fcs_gen.generate(\n",
    "    sample['audio_path'],\n",
    "    sample['question'],\n",
    "    sample['task'],\n",
    "    sample['options'],\n",
    "    original_pred\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(fcs_samples)} FCS samples\")\n",
    "print(f\"\\nOriginal question: {sample['question']}\")\n",
    "print(f\"\\nFCS text variations (complementary):\")\n",
    "unique_fcs_questions = set(s['question'] for s in fcs_samples)\n",
    "for i, q in enumerate(unique_fcs_questions, 1):\n",
    "    print(f\"  {i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FCS predictions\n",
    "print(\"Getting predictions on FCS samples...\")\n",
    "fcs_predictions = []\n",
    "\n",
    "for fcs_sample in tqdm(fcs_samples[:10], desc=\"FCS predictions\"):\n",
    "    pred, _ = model.predict(\n",
    "        fcs_sample['audio_path'],\n",
    "        fcs_sample['question'],\n",
    "        fcs_sample['options']\n",
    "    )\n",
    "    fcs_predictions.append(pred)\n",
    "\n",
    "print(f\"\\nOriginal prediction: {original_pred}\")\n",
    "print(f\"FCS predictions: {Counter(fcs_predictions)}\")\n",
    "\n",
    "# Should be different from original\n",
    "sensitivity = sum(1 for p in fcs_predictions if p != original_pred) / len(fcs_predictions)\n",
    "print(f\"\\nSensitivity (% different): {sensitivity:.1%}\")\n",
    "print(f\"Model is {'SENSITIVE' if sensitivity > 0.5 else 'INSENSITIVE'} to complementary samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stochastic Sampling Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test stochastic sampling for output entropy baseline\n",
    "sample_probs = model.predict_with_sampling(\n",
    "    sample['audio_path'],\n",
    "    sample['question'],\n",
    "    sample['options'],\n",
    "    num_samples=20,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(f\"Stochastic sampling results (20 samples, T=0.7):\")\n",
    "for option, prob in sorted(sample_probs.items()):\n",
    "    bar = '█' * int(prob * 50)\n",
    "    print(f\"  ({option}) {prob:.4f} {bar}\")\n",
    "\n",
    "# Compute entropy\n",
    "from src.baselines import BaselineUncertainty\n",
    "baseline = BaselineUncertainty()\n",
    "\n",
    "# Generate samples based on probabilities\n",
    "samples = []\n",
    "for option, prob in sample_probs.items():\n",
    "    samples.extend([option] * int(prob * 20))\n",
    "\n",
    "entropy = baseline.output_entropy(samples)\n",
    "print(f\"\\nOutput Entropy: {entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tested the Qwen2-Audio model:\n",
    "- ✅ Loaded model successfully\n",
    "- ✅ Tested basic inference\n",
    "- ✅ Computed baseline accuracy per task\n",
    "- ✅ Tested FES generator and consistency\n",
    "- ✅ Tested FCS generator and sensitivity\n",
    "- ✅ Tested stochastic sampling\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run full FESTA evaluation (notebook 03)\n",
    "2. Compare with baseline methods\n",
    "3. Analyze uncertainty calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary files\n",
    "fes_gen.cleanup_temp_files()\n",
    "fcs_gen.cleanup_temp_files()\n",
    "print(\"Cleaned up temporary files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
